





import os


SSD_PATH = '/Volumes/PortableSSD'
PROJECT_PATH = f'{SSD_PATH}/Projects/kaggle-project'
RESULTS_PATH = f'{SSD_PATH}/Projects/results'
MODELS_PATH = f'{SSD_PATH}/Projects/models'


os.makedirs(RESULTS_PATH, exist_ok=True)
os.makedirs(MODELS_PATH, exist_ok=True)


import re
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import torch
from transformers import (
    pipeline,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from datasets import Dataset

from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, average_precision_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer





# 데이터 로드
train_df = pd.read_csv('../data/jigsaw_agile/train.csv')
test_df = pd.read_csv('../data/jigsaw_agile/test.csv')

# 데이터 크기 확인
print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")


train_df.info()


train_df.head()


# 클래스 불균형 확인
train_df["rule_violation"].value_counts()





def preprocess_text(text):
    # 소문자로 통일
    text = str(text).lower()

    # url이 포함된 경우 [URL] 토큰으로 치환
    text = re.sub(r'http\S+|www\S+', '[URL]', text)

    # 특수문자 제거
    text = re.sub(r'[^\w\s[\]]', '', text)

    # 공백 제거
    text = ' '.join(text.split())
    return text


body_processed = train_df["body"].apply(preprocess_text)
rule_processed = train_df["rule"].apply(preprocess_text)

# 타겟 컬럼 분리
X = body_processed + " [RULE] " + rule_processed
y = train_df["rule_violation"]

# 테스트 데이터에도 동일한 전처리 적용
body_processed_test = test_df["body"].apply(preprocess_text)
rule_processed_test = test_df["rule"].apply(preprocess_text)

X_test = body_processed_test + " [RULE] " + rule_processed_test

# 전처리된 샘플 데이터를 확인합니다.
print(f"Processed data sample: {X.iloc[0][:100]}")


# 데이터 분할
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


print(X)





# TF-IDF 시행
tfidf = TfidfVectorizer(
    max_features=2000,
    min_df=2,
    max_df=0.8,
    ngram_range=(1,3),
    sublinear_tf=True,
    use_idf=True
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_val_tfidf = tfidf.transform(X_val)

print(f'TF-IDF 벡터 shape:')
print(f'  Train: {X_train_tfidf.shape}')
print(f'  Test: {X_val_tfidf.shape}')


from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC

models_sklearn = {
    "Logistic Regression": LogisticRegression(max_iter=2000, random_state=42, n_jobs=-1),
    "LinearSVC": LinearSVC(class_weight="balanced"),
    "SGD-Log": SGDClassifier(loss="log_loss", class_weight="balanced", max_iter=2000, random_state=42, n_jobs=-1),
}

for name, sk_model in models_sklearn.items():
    sk_model.fit(X_train_tfidf, y_train)
    y_tr_pred = sk_model.predict(X_train_tfidf)
    y_val_pred = sk_model.predict(X_val_tfidf)

    train_accuracy = accuracy_score(y_train, y_tr_pred)
    test_accuracy = accuracy_score(y_val, y_val_pred)
    test_f1 = f1_score(y_val, y_val_pred)

    print(f"Model: {name}")
    print(f"Train_accuracy: {train_accuracy:.3f}")
    print(f"Val_accuracy: {test_accuracy:.3f}")
    print(f"Val_f1_score: {test_f1:.3f}")
    print()








def finetune_model(model_name, X_train, y_train, X_val, y_val, epochs=1):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        id2label={0:"not_violation", 1:"violation"},
        label2id={"not_violation":0, "violation":1}
    )

    tok_tr = tokenizer(list(X_train), truncation=True, max_length=128)
    tok_val = tokenizer(list(X_val), truncation=True, max_length=128)
    
    tok_tr["labels"] = y_train.tolist()
    tok_val["labels"] = y_val.tolist()
    
    ds_tr = Dataset.from_dict(tok_tr)
    ds_val = Dataset.from_dict(tok_val)

    collator = DataCollatorWithPadding(tokenizer=tokenizer)

    def metrics(p):
        logits, labels = p
        probs = torch.softmax(torch.tensor(logits), dim=-1)[:,1].numpy()
        preds = (probs >= 0.5).astype(int)
        return {
            "accuracy":  accuracy_score(labels, preds),
            "f1":        f1_score(labels, preds),
            "roc_auc":   roc_auc_score(labels, probs),
            "pr_auc":    average_precision_score(labels, probs),
        }

    args = TrainingArguments(
        output_dir=f'{RESULTS_PATH}/{model_name.replace("/", "_")}',
        num_train_epochs=epochs,
        per_device_train_batch_size=8, 
        per_device_eval_batch_size=16,
        gradient_accumulation_steps=2,
        learning_rate=3e-5,
        eval_strategy='epoch', 
        save_strategy='epoch',   
        load_best_model_at_end=True,
        metric_for_best_model='f1',
        weight_decay=0.01,
        fp16=False,
        logging_steps=100,
        seed=42,
        use_cpu=True,
        no_cuda=True,
        report_to='none',
        disable_tqdm=False,
    )
    
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=ds_tr,
        eval_dataset=ds_val,  
        tokenizer=tokenizer,
        data_collator=collator,
        compute_metrics=metrics
    )
    
    trainer.train()

    pred = trainer.predict(ds_val)
    probs = torch.softmax(torch.tensor(pred.predictions), dim=-1)[:,1].numpy()
    preds = (probs >= 0.5).astype(int)
    
    return trainer, tokenizer, ds_val, probs, preds


transformer_model_ids = [
    "distilroberta-base",
    "bert-base-uncased",
    "distilbert-base-uncased",
]

results = {}

for tf_model in transformer_model_ids:
    trainer, tokenizer, ds_val, probs, preds = finetune_model(
    tf_model, X_train, y_train, X_val, y_val, epochs=3
)
    results[tf_model] = {
        'f1': f1_score(y_val, preds),
        'accuracy': accuracy_score(y_val, preds),
        'roc_auc': roc_auc_score(y_val, preds)
    }
    
    print(f"Model: {tf_model}")
    print(f" - F1: {results[tf_model]['f1']:.4f}")
    print(f" - Accuracy: {results[tf_model]['accuracy']:.4f}")
    print(f" - ROC-AUC: {results[tf_model]['roc_auc']:.4f}")


















